{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y658KIJfRT_i"
      },
      "source": [
        "#Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from google.colab import files # модуль для загрузки файлов в colab\n",
        "import numpy as np #библиотека для работы с массивами данных\n",
        "import os\n",
        "import json\n",
        "from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Flatten # из кераса загружаем необходимые слои для нейросети\n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta # из кераса загружаем выбранный оптимизатор\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки\n",
        "from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml # импортируем модуль для удобной работы с файлами\n",
        "import tensorflow as tf\n",
        "from flask import Flask, request\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "gEj9_k83Jbwz"
      },
      "outputs": [],
      "source": [
        "ques = np.load('questions.npy')\n",
        "ans = np.load('answers.npy')\n",
        "questions1 = list(ques)\n",
        "answers1 = list(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXT_VLHrXMM-",
        "outputId": "85f1228d-1819-47a3-ea82-0e6042c5bd2d"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# Подключаем керасовский токенизатор и собираем словарь индексов\n",
        "######################\n",
        "tokenizer = Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,  char_level=False, oov_token='unknown')\n",
        "tokenizer.fit_on_texts(questions1 + answers1) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности\n",
        "vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря\n",
        "vocabularySize = len(vocabularyItems)+1 # размер словаря\n",
        "#print( 'Фрагмент словаря : {}'.format(vocabularyItems[:150]))\n",
        "#print( 'Размер словаря : {}'.format(vocabularySize))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "maxLenQuestions = 11\n",
        "maxLenAnswers = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "5-DxR17_dvXr"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# Первый входной слой, кодер, выходной слой\n",
        "######################\n",
        "encoderInputs = Input(shape=(11, )) # размеры на входе сетки (здесь будет encoderForInput)\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "encoderEmbedding = Embedding(vocabularySize, 200,  mask_zero=True) (encoderInputs) # x_train  maskzero=rue при обучнии не берет нули\n",
        "# Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c\n",
        "# Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже\n",
        "encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)\n",
        "encoderStates = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "KvDe8JNad4qM"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# Второй входной слой, декодер, выходной слой\n",
        "######################\n",
        "decoderInputs = Input(shape=(11, )) # размеры на входе сетки (здесь будет decoderForInput)\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "# mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: \"У меня все хорошо PAD PAD PAD PAD PAD PAD..\"\n",
        "decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) \n",
        "# Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c\n",
        "\n",
        "decoderLSTM = LSTM(200, return_state=True, return_sequences=True)\n",
        "\n",
        "decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)\n",
        "# И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе\n",
        "decoderDense = Dense(vocabularySize, activation='softmax') \n",
        "output = decoderDense (decoderOutputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoderInputs (InputLayer)      [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoderInputs (InputLayer)      [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 11, 200)      936200      encoderInputs[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 11, 200)      936200      decoderInputs[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 11, 200), (N 320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 11, 4681)     940881      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 3,454,881\n",
            "Trainable params: 3,454,881\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = load_model('model2.h5')\n",
        "#filename = \"weights-improvement-13-0.0002.hdf5\"\n",
        "#model.load_weights(filename)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swbRHIZGel0E"
      },
      "source": [
        "#Подготовка и запуск нейросети для генерации ответов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "K6nvRDTBev8D"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов\n",
        "######################\n",
        "def strToTokens(sentence: str): # функция принимает строку на вход (предложение с вопросом)\n",
        "  words = sentence.lower().split() # приводит предложение к нижнему регистру и разбирает на слова\n",
        "  tokensList = list() # здесь будет последовательность токенов/индексов\n",
        "  for word in words: # для каждого слова в предложении\n",
        "    tokensList.append(tokenizer.word_index[word]) # определяем токенизатором индекс и добавляем в список\n",
        "\n",
        "    # Функция вернёт вопрос в виде последовательности индексов, ограниченной длиной самого длинного вопроса из нашей базы вопросов\n",
        "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Bj6wv9NnhL9w"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# Устанавливаем связи между слоями рабочей модели и предобученной\n",
        "######################\n",
        "def loadInferenceModels():\n",
        "  encoderInputs = model.input[0]   # входом энкодера рабочей модели будет первый инпут предобученной модели(input_1)\n",
        "  encoderEmbedding = model.layers[2] # связываем эмбединг слои(model.layers[2] это embedding_1)\n",
        "  encoderOutputs, state_h_enc, state_c_enc = model.layers[4].output # вытягиваем аутпуты из первого LSTM слоя обуч.модели и даем энкодеру(lstm_1)\n",
        "  encoderStates = [state_h_enc, state_c_enc] # ложим забранные состояния в состояния энкодера\n",
        "  encoderModel = Model(encoderInputs, encoderStates) # формируем модель\n",
        "\n",
        "  decoderInputs = model.input[1]   # входом декодера рабочей модели будет второй инпут предобученной модели(input_2)\n",
        "  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h\n",
        "  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c\n",
        "\n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
        "\n",
        "  decoderEmbedding = model.layers[3] # связываем эмбединг слои(model.layers[3] это embedding_2)\n",
        "  decoderLSTM = model.layers[5] # связываем LSTM слои(model.layers[5] это lstm_2)\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding.output, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
        "\n",
        "  decoderDense = model.layers[6] # связываем полносвязные слои(model.layers[6] это dense_1)\n",
        "  decoderOutputs = decoderDense(decoderOutputs) # выход с LSTM мы пропустим через полносвязный слой с софтмаксом\n",
        "\n",
        "    # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
        "    # на выходе предсказываемый ответ и новые состояния\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "  return encoderModel , decoderModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "#warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import warnings \n",
        "#warnings.filterwarnings('ignore')\n",
        "args = ' '\n",
        "#decod_list = (answer_bot(args))\n",
        "#json.loads(decod_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyGCrp5qhPpm",
        "outputId": "940682c7-014f-4dd3-f212-6f1b220a5de2"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем рабочую модель над предобученной\n",
        "######################\n",
        "def answer_bot(args):\n",
        "    encModel , decModel = loadInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "    \n",
        "    #for _ in range(1): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "    # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "    s_token = strToTokens(args)\n",
        "    statesValues = encModel.predict(strToTokens(args)) #(input( 'Задайте вопрос : ' )\n",
        "    # Создаём пустой массив размером (1, 1)\n",
        "    emptyTargetSeq = np.zeros((1, 1))    \n",
        "    emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "    stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "    decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "    while not stopCondition : # пока не сработало стоп-условие\n",
        "      # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "      # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "      decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "      \n",
        "      #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "      sampledWordIndex = np.argmax(decOutputs, axis=-1) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "      sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "      for word , index in tokenizer.word_index.items():\n",
        "        if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "          decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "          sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "      \n",
        "      # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "      if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "        stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "      emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "      emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "      statesValues = [h, c] # и состояния, обновленные декодером\n",
        "      # и продолжаем цикл с обновленными параметрами\n",
        "      data = decodedTranslation[:-3]\n",
        "      answers = json.dumps({\"data\":data})\n",
        "      ans_b = json.loads(answers)\n",
        "    return (ans_b['data'])             #print(decodedTranslation[:-3])#decodedTranslation[:-3] # выводим ответ сгенерированный декодером"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='decoderInputs'), name='decoderInputs', description=\"created by layer 'decoderInputs'\"), but it was called on an input with incompatible shape (None, 1).\n",
            " этот товар абатмент осстем зиоцера стоит 26235 \n"
          ]
        }
      ],
      "source": [
        "args = 'сколько стоит абатмент осстем конвертибл'\n",
        "ans = answer_bot(args)\n",
        "print(ans)\n",
        "#an = json.loads(ans)\n",
        "#print(an['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='decoderInputs'), name='decoderInputs', description=\"created by layer 'decoderInputs'\"), but it was called on an input with incompatible shape (None, 1).\n",
            " этот товар абатмент осстем зиоцера стоит 26235 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def new_func(current_text):\n",
        "    args_answer = current_text\n",
        "    ans_bot = json.loads(answer_bot(args_answer))\n",
        "    An_Data = ans_bot['data']\n",
        "    return An_Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Курсовая работа. Чат-Бот.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
